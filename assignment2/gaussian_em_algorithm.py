# -*- coding: utf-8 -*-
"""Gaussian_EM_Algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FQO6lVIs1Sk13TvmnCspT3OVxJA5lLWI
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import pprint
import matplotlib.pyplot as plt

# %matplotlib inline
#%precision 4
#np.set_printoptions(precision=4)
import pylab as pl

from google.colab import drive
drive.mount("/content/gdrive")
#on running this it will ask permission to mount with google drive. please click ok and allow. this program does not access any of the
# google drive documents except dataset which is uploaded

#from google.colab import files
#uploaded = files.upload()
#for func in uploaded.keys():
#       print('User uploaded file "{name}"'.format(name=func))

import warnings

#suppress warnings
warnings.filterwarnings('ignore')

from numpy import genfromtxt
mylist=genfromtxt("/content/gdrive/My Drive/Colab Notebooks/A2Q1.csv", delimiter=',')
print(mylist)

import random
def kmeans():
  initialpartition = []
  for i in range(mylist.shape[0]):
    n = random.randint(1,4)
    initialpartition.append(n)
  #print(initialpartition)

  sum=np.zeros((4,mylist.shape[1]))
  count=np.zeros((4,mylist.shape[1]))
  for i in range(len(initialpartition)):
    sum[initialpartition[i]-1]+=mylist[i]
    count[initialpartition[i]-1]+=1
  #print(sum.shape)
  mean=np.zeros((4,mylist.shape[1]))
  for i in range(4):
    for j in range(mylist.shape[1]):
      if(count[i][j]==0):
        mean[i][j]=0
      else:
        mean[i][j]=sum[i][j]/count[i][j]
  #mean
  dist=np.zeros(4)
  newpartition=[]
  errorfunction=[]
  c=0
  prevpartition=initialpartition
  while prevpartition!=newpartition:
    prevpartition=newpartition
    newpartition=[]
    s=0
    for i in range(mylist.shape[0]):
      for j in range(4):
        dist[j]=np.linalg.norm(mylist[i]-mean[j])
      mink=np.argmin(dist)
      newpartition.append(mink+1)
      s+=(dist[mink])
    errorfunction.append(s)

    sum=np.zeros((4,mylist.shape[1]))
    count=np.zeros((4,mylist.shape[1]))
    for i in range(len(newpartition)):
      sum[newpartition[i]-1]+=mylist[i]
      count[newpartition[i]-1]+=1

    for i in range(4):
      for j in range(mylist.shape[1]):
        if(count[i][j]==0):
          mean[i][j]=0
        else:
          mean[i][j]=sum[i][j]/count[i][j]
    c=c+1;
  
  iterations=[i for i in range(1,c+1)]   
 
  return (mean,newpartition)
  


  
(means,initialpartition)=kmeans()

pi=np.zeros(4)
for k in range(len(initialpartition)):
  if(initialpartition[k]==1):
    pi[0]+=1
  elif (initialpartition[k]==2):
    pi[1]+=1
  elif (initialpartition[k]==3):
    pi[2]+=1
  elif(initialpartition[k]==4):
    pi[3]+=1

pi=pi/mylist.shape[0]
pi=pi/pi.sum(axis=0,keepdims=1)

variances = [np.zeros((50,50))] * 4

for k in range(4):
  numer=np.zeros((50,50))
  for i in range(mylist.shape[0]):
    temp=np.outer((mylist[i]-means[k]).T,(mylist[i]-means[k]))
    numer+=temp

  variances[k]=numer/mylist.shape[0]



import math
def calculate_log(means,variances):
  total_sum=0
  for i in range(400):
    sum=0
    for k in range(4):
        v1=np.exp(-0.5*(np.dot((np.matmul((mylist[i]-means[k]),np.linalg.pinv(variances[k]))), (mylist[i]-means[k]).T)))   
        eigen_values , eigen_vectors = np.linalg.eig(variances[k])
        determinant=1
        for eigenvalue in eigen_values:
          if(eigenvalue>0.00001):
            determinant*=eigenvalue

        temp=pow(2*np.pi,25)
        temp*=np.sqrt(determinant)
        numerator=(v1*pi[k]/temp)
        sum+=numerator

    total_sum+=math.log(sum)
  return total_sum

import math
def Estep(means,variances,pi):
  lambda_t=np.zeros((400,4))
  total_sum=0
  for i in range(400):
    sum=0
    for k in range(4):
      v1=np.exp(np.array((-0.5*(np.matmul(np.matmul((mylist[i]-means[k]),np.linalg.pinv(variances[k])), ((mylist[i]-means[k]).T)))),dtype=np.float128))   
      eigen_values , eigen_vectors = np.linalg.eig(variances[k])
      determinant=1
      for eigenvalue in eigen_values:
        if(eigenvalue>0.00001):
          determinant*=eigenvalue

      temp=pow(2*np.pi,25)
      temp*=np.sqrt(determinant)
      if(temp!=0):
        sum+=(v1*pi[k]/temp)

    total_sum+=math.log(sum)

    for k in range(4):
      v1=np.exp(np.array((-0.5*(np.matmul(np.matmul((mylist[i]-means[k]),np.linalg.pinv(variances[k])), ((mylist[i]-means[k]).T)))),dtype=np.float128))
      eigen_values , eigen_vectors = np.linalg.eig(variances[k])
      determinant=1
      for eigenvalue in eigen_values:
        if(eigenvalue>0.00001):
          determinant*=eigenvalue

      temp=pow(2*np.pi,25)
      temp*=np.sqrt(determinant)
      if(temp!=0):
        numerator=(v1*pi[k]/temp)
      if(sum!=0):
        lambda_t[i][k]=(numerator/sum)

  
  return (lambda_t,total_sum)

def Mstep(lambda_t):
  means=np.zeros((4,50))
  variances = [np.zeros((50,50))] * 4
  pi=np.zeros(4)
  for k in range(4):
    denom=0
    for i in range(mylist.shape[0]):
      denom+=lambda_t[i][k]

    sum2=np.zeros(50)
    for i in range(mylist.shape[0]):
      sum2+=lambda_t[i][k]*mylist[i]

    
    if(denom!=0):
     means[k]=sum2/denom
  

    pi[k]=denom/mylist.shape[0]
    numer=np.zeros((50,50))
    for i in range(mylist.shape[0]):
      temp=np.outer((mylist[i]-means[k]).T,(mylist[i]-means[k]))
      temp=lambda_t[i][k]*temp
      numer+=temp
      
    if(denom!=0 and np.linalg.norm(numer)!=math.inf):
      variances[k]=numer/denom
    

  return (means,variances,pi)

piprev=np.zeros(4)
prevmeans=np.zeros((4,50))
prevvariances=[np.zeros((50,50))] * 4
max_log_likelihood=[]
prev_log_likelihood=-1
log_likelihood=100
iterations=[]
i=0
flag=1
while(flag>0.001):
  prev_log_likelihood=log_likelihood
  (lambda_t,log_likelihood)=Estep(means,variances,pi)
  prevmeans=means
  prevvariances=variances
  piprev=pi
  iterations.append(i)
  i+=1
  (means,variances,pi) = Mstep(lambda_t)
  flag=(np.linalg.norm(pi-piprev))
  
  max_log_likelihood.append(log_likelihood)

plt.plot(iterations,max_log_likelihood)
plt.xlabel("iterations")
plt.ylabel("loglikelihood")
plt.show()